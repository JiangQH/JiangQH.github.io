---
layout: post
title: 从最优化间隔器到核函数---SVM的推论（上）
category: ml
description: svm, 分类问题
keywords: svm
---
本文以及接下来的一篇文章将尝试详细的去描述SVM（支持向量机）这一经典的分类算法。SVM曾经被认很多人认为是最好的离线监督学习算法（不需要做什么修改即可使用），可以说至少在CV识别领域，在2012年的ImageNet比赛Alex使用CNN横空出世夺得第一之前，有长达10来年一直是kernel methods的天下。

本文以及接下来的文章将由简入繁一步步引入最终模型：它们分别是线性可分支持向量机，线性支持向量机以及非线性支持向量机，最后还将引入讨论序列最小最优化（SMO）算法，它是一个求解SVM的解的高效算法，该算法在1998年由微软研究院的Platt提出。

本文将讨论线性可分支持向量机和线性支持向量机，并导出它们的对偶形式。下文将引入核方法，使得SVM可以应用到非线性可分的情况中去，并讨论SMO算法。

在正式讨论前，先来给定我们的数据情况。给定一个特征空间上的训练集

$$S = \{(x^{i}, y^{i}), i=1,2,...m\}$$

其中，$x^{i} \epsilon R^{n}, y^{i} \epsilon\{+1,-1\}$。称$(x^{i}, y^{i})$为一个样本点，当$y^{i}$为正时称$x^{i}$为正例，反之为负例。

## 一、线性可分支持向量机（硬间隔最大化）

我们首先来讨论线性可分的情况，对线性可分的一个很直观的解释即在特征空间内存在一个超平面可以将正负两类完全分开。下图展示了特征空间为2维的线性可分情况。

<div style="text-align: center">![svm1](/images/1112/1.png) <div>

图中的黑色实线为其中一个解，它将我们的正负例进行了分开。我们定义该直线的表达式为

$$w^{T}x + b = 0$$

可以得到叉叉表示的正例点$x^{i}$ 满足$w^{T}x^{i} + b > 0$，而圆圈表示的负例点$x^{i}$满足$w^{T}x^{i} + b < 0$。所以我们定义分类器为：

$$h_{w,b} (x)= g(w^{T}x + b) $$

这里$g(z)=1,$ 如果$z \geq 0$；否则$g(z) = -1。即直线上侧的点被标记为正例，下侧的点被标记为负例。

先结合上图来看一个关于该分类器的直观感受。我们知道当点距离超平面越远，将该点带入超平面方程后得到的值的绝对值$\|w^{T}x + b\|$就越大（因为有负，所以说绝对值）。所以我们可以用一个点到分类面的远近来表示我们对分类问题的确信度。反应到分类器上有样本点$(x^{i},y^{i})$的确性度以及正确性可以用$y^{i}(w^{T}x^{i} + b)$来表示，这是因为$w^{T}x^{i} + b$与$y^{i}$的符号可以表示我们的分类正确与否， 而上述绝对值又可以表明我们在正确分类下的确性度。可以看到在上图中，因为紫色点距离分类面较近，所以预测不那么确信，对应的值也较小；而红色点距离分类面较远，我们比较确信，对应的值也就较大。由这个直观感受，我们引入函数间隔以及几何间隔的概念。

### a、函数间隔与几何间隔

我们将之前的表述进行形式化，
。。。未完待续


