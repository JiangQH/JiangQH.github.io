---
layout: post
title: 卷积层实现以caffe为例
category: ml
description: dl
keywords: 卷积层实现
---
前段时间尝试去重现了下卷积层的具体实现，网上看了些blog，但是发现的现状是好多文章都是相互抄袭，到最后自己都不知道讲什么。于是自己详细梳理了下卷积层的具体实现，并以caffe为例详细了解下代码实现。

## 卷积层的基本原理与加速
我们知道从filter的角度来看待Convolution层，其实就是我们用不同的filter来对输入层做卷积生成一个输出。具体来说如下图：  
![conv](/images/0311/conv.jpg)  
假设Input的大小为\\(\(C, H, W\)\\)，其中\\(C\\)为channel数，\\(H\\)为高度，\\(W\\)为宽度。  
定义filter的大小为\\(\(F_h, F_w\)\\)，那么filter的channel数是默认为\\(C\\)的。并且定义\\(pad\\)的大小为\\(P\\), \\(stride\\)的大小为\\(S\\)。
那么output的得到过程是这样的，我们将filter放在左上方初始的位置，然后每次以\\(stride\\)大小进行移动，每次将filter与Input对应的部分进行卷积，得到一个output输出点---如上图所示每个output都与一个Input区域一一对应，这样我们便生成了一个feature_map，将\\(K\\)个filter的结果结合起来我们便得到了一个输出\\(K, H_{new}, W_{new}\\)。并且我们可以根据上述过程推导得到output的大小：  
$$H_{new} = \frac{H + 2P - F}{S} + 1$$  
$$W_{new} = \frac{W + 2P - F}{S} + 1$$  
有兴趣的朋友可以自己去推导一下，一个更形象的filter过程可以参考[cs231n](http://cs231n.github.io/)中的一个例子，如下所示：  
<iframe height="700px;" width="100%" src="http://cs231n.github.io/assets/conv-demo/index.html" style="border:none;"></iframe>  
那么当我们想要计算输出的时候只需要模拟上述过程不断地用filter去输入上进行卷积即可得到输出。  
但是我们不要忘了所谓的卷积只是矩阵乘法而已，能不能进行加速呢？答案是可以的！如下以一个filter为例子所示：  
![filter](/images/0311/filter.jpg)  
上图解释了output的生成过程，注意到**output上的每一个点都对应于Input上不同的位置与相同的filter做矩阵乘法的结果**---这就是我们加速的关键之处---将所有的可能Input位置一次性找出来，然后与filter做矩阵乘法，一步得到output输出！而K个filter的情况不过是一个拓展而已。  
具体的一个例子：  
如果我们的Input大小为\\(\(3, 227, 227\)\\)，我们以一个\\(\(3, 11, 11\)\\)的filter对其进行卷积，\\(stride\\)大小为4。我们将每次与filter进行计算的输入区域展开为一个列向量，那么其大小为\\(3 \times 11 \times 11 = 363\\)，在Input上共有\\(H_{new} = \frac{227 - 11}{4} + 1 = 55, W_{new} = \frac{227 - 11}{4} + 1 = 55\\)个参加计算的位置，一共\\(55 \times 55 = 3025\\)将这些列向量进行拼接我们便得到了一个矩阵**\\(x_{col}\\)**，其大小为\\(\[363 \times \ 3025]\\)，这一过程叫做**im2col**  
同时我们将每一个filter展开为一个行向量，大小为\\(363\\)，假设我们共有\\(K\\)个filter，那么我们得到另一个矩阵\\(W_{row}\\)，大小为\\(\[K \times 363\]\\)  
将这两个矩阵相乘\\(W_{row} \times x_{col}\\)，得到output，大小为\\(\[K \times 3025\]\\),再resize回去即可得到我们的输出。  
可以看到我们只进行了一次矩阵的乘法便得到了输出，唯一的缺点在于由于\\(x_{col}\\)中有大量的重复数据，我们会占用更多的内存。但是\\(x_{col}\\)的引入除了加速外也使得前向反向传播变得很清晰。  
考虑从上一层反向传播过来的\\(diff = \frac{\partial Loss}{\partial output}\\),当我们要进行反向传播和参数更新时十分直观:  
$$\frac{\partial Loss}{\partial W_{row}} = diff \times \frac{\partial out}{\partial W_{row}} = diff \times x_{col}$$  
对于反向传播因为前向传播时只有\\(x_{col}\\)参与其中，那么反向传播时也只有这些数据会被传播到，所以
$$\frac{\partial Loss}{\partial input} = diff \times \frac{\partial out}{\partial x_{col}} = diff \times W_{row}$$  
可以看到\\(x_{col}\\)的引入是十分有利的，而这也是现在大多数深度学习框架对Convolution层的具体实现方式，下面以caffe为例子，从代码层面进行具体的阐述。

## caffe中的卷积层实现  
//todo






